"""Updated CLI with subtask-centric Virtual Lab integration."""

import argparse
import os
import sys
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

from src.agent.agent import create_agent
from src.agent.meeting import run_virtual_lab
from src.agent.meeting_refactored import run_virtual_lab as run_virtual_lab_subtask
from src.virtuallab_workflow.workflow import (
    run_consensus_workflow,
    run_research_workflow,
)


def save_answer_to_file(
    answer: str, question: str, output_path: str = None, mode: str = "single"
) -> str:
    """Save the final answer to a markdown file."""
    from src.utils.output_manager import get_current_run_dir

    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Use OUTPUT_DIR if available
        run_dir = get_current_run_dir()
        if run_dir:
            output_path = run_dir / f"answer_{timestamp}.md"
        else:
            output_path = f"answer_{timestamp}.md"

    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    content = f"""# CoScientist Research Report
**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Mode:** {mode.replace("-", " ").title()}

---

## Research Question

{question}

---

## Final Answer

{answer}

---

*Generated by CoScientist - AI Research Assistant*
"""

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(content)

    return str(output_file.absolute())


def main():
    """Main CLI entry point."""

    parser = argparse.ArgumentParser(
        description="CoScientist: AI Research Assistant for Biomedical Questions",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Subtask-Centric Virtual Lab (NEW - RECOMMENDED)
  python -m src.cli_updated --question "..." --subtask-centric --rounds 1 --team-size 3 --verbose

  # Original Virtual Lab (parallel)
  python -m src.cli_updated --question "..." --virtual-lab --rounds 2 --team-size 3

  # Combined Mode (LangGraph + Consensus)
  python -m src.cli_updated --question "..." --combined --rounds 2 --team-size 3

  # Compare old vs new Virtual Lab
  python -m src.cli_updated --question "..." --compare-vl

  # Single agent
  python -m src.cli_updated --question "..."
        """,
    )

    parser.add_argument("--question", "-q", type=str, help="Question to ask the agent")
    parser.add_argument(
        "--interactive", "-i", action="store_true", help="Run in interactive mode"
    )
    parser.add_argument(
        "--model",
        "-m",
        type=str,
        default=os.getenv("OPENROUTER_MODEL", "anthropic/claude-sonnet-4"),
        help="Model to use",
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Print verbose output"
    )
    parser.add_argument(
        "--with-critic", "-c", action="store_true", help="Enable critic feedback"
    )
    parser.add_argument(
        "--virtual-lab",
        "-vl",
        action="store_true",
        help="Original Virtual Lab (parallel)",
    )
    parser.add_argument(
        "--subtask-centric",
        "-sc",
        action="store_true",
        help="NEW: Subtask-Centric Virtual Lab (sequential, recommended)",
    )
    parser.add_argument(
        "--compare-vl",
        action="store_true",
        help="Compare old vs new Virtual Lab side-by-side",
    )
    parser.add_argument(
        "--combined", action="store_true", help="Combined Mode (LangGraph + Consensus)"
    )
    parser.add_argument("--langgraph", action="store_true", help="LangGraph workflow")
    parser.add_argument(
        "--rounds",
        "-r",
        type=int,
        default=2,
        help="Number of rounds (default: 2 for old VL, 1 for subtask-centric)",
    )
    parser.add_argument(
        "--team-size", "-t", type=int, default=3, help="Max team size (default: 3)"
    )
    parser.add_argument("--api-key", type=str, help="API key")
    parser.add_argument(
        "--data-dir",
        "-d",
        type=str,
        default=os.getenv(
            "DATABASE_DIR", "/home.galaxy4/sumin/project/aisci/Competition_Data"
        ),
        help="Path to database directory",
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        default=os.getenv("INPUT_DIR"),
        help="Path to question-specific input data",
    )
    parser.add_argument("--output", "-o", type=str, help="Output file path")
    parser.add_argument(
        "--s2-keywords",
        action="store_true",
        default=True,
        help="Use keyword extraction for Semantic Scholar (default: True)",
    )
    parser.add_argument(
        "--no-s2-keywords",
        dest="s2_keywords",
        action="store_false",
        help="Disable keyword extraction, use full question for Semantic Scholar",
    )

    args = parser.parse_args()

    if args.input_dir is None:
        args.input_dir = args.data_dir

    # Set s2_use_keywords in config
    from src.config import get_global_config

    config = get_global_config()
    config.s2_use_keywords = args.s2_keywords

    # Determine provider
    provider = "openrouter" if args.model and "/" in args.model else "anthropic"

    # Handle subtask-centric mode
    if args.subtask_centric:
        print("\n" + "=" * 60)
        print("SUBTASK-CENTRIC VIRTUAL LAB (NEW)")
        print("=" * 60)
        print(f"Question: {args.question}")
        print(
            f"Configuration: {args.rounds} round(s), max {args.team_size} specialists"
        )
        print("Mode: Sequential execution with Red Flag enforcement")
        print("=" * 60)

        # Use 1 round by default for subtask-centric (more efficient)
        rounds = args.rounds if args.rounds != 2 else 1  # Override default 2 to 1

        final_answer = run_virtual_lab_subtask(
            question=args.question,
            api_key=args.api_key,
            model=args.model,
            provider=provider,
            num_rounds=rounds,
            max_team_size=args.team_size,
            verbose=args.verbose,
            data_dir=args.data_dir,
            input_dir=args.input_dir,
        )

        print("\n" + "=" * 60)
        print("FINAL ANSWER (With Red Flag Resolution):")
        print("=" * 60)
        print(final_answer)

        output_file = save_answer_to_file(
            final_answer, args.question, args.output, mode="subtask-centric-vl"
        )
        print(f"\n✓ Answer saved to: {output_file}")
        return

    # Handle comparison mode
    if args.compare_vl:
        print("\n" + "=" * 80)
        print("COMPARING: Original VL (Parallel) vs Subtask-Centric VL (Sequential)")
        print("=" * 80)

        # Run old version
        print("\n[1/2] Running Original Virtual Lab...")
        old_answer = run_virtual_lab(
            question=args.question,
            api_key=args.api_key,
            model=args.model,
            provider=provider,
            num_rounds=args.rounds,
            max_team_size=args.team_size,
            verbose=False,  # Suppress verbose for comparison
            data_dir=args.data_dir,
            input_dir=args.input_dir,
        )

        # Run new version
        print("\n[2/2] Running Subtask-Centric Virtual Lab...")
        new_answer = run_virtual_lab_subtask(
            question=args.question,
            api_key=args.api_key,
            model=args.model,
            provider=provider,
            num_rounds=1,  # Subtask-centric uses 1 round
            max_team_size=args.team_size,
            verbose=False,
            data_dir=args.data_dir,
            input_dir=args.input_dir,
        )

        # Display comparison
        print("\n" + "=" * 80)
        print("COMPARISON RESULTS")
        print("=" * 80)

        print("\n" + "-" * 80)
        print("ORIGINAL VIRTUAL LAB (Parallel):")
        print("-" * 80)
        print(old_answer[:1000] + "..." if len(old_answer) > 1000 else old_answer)

        print("\n" + "-" * 80)
        print("SUBTASK-CENTRIC VIRTUAL LAB (Sequential):")
        print("-" * 80)
        print(new_answer[:1000] + "..." if len(new_answer) > 1000 else new_answer)

        # Save both
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        old_file = save_answer_to_file(
            old_answer, args.question, f"comparison_old_{timestamp}.md", "original-vl"
        )
        new_file = save_answer_to_file(
            new_answer,
            args.question,
            f"comparison_new_{timestamp}.md",
            "subtask-centric-vl",
        )

        print(f"\n✓ Original VL saved to: {old_file}")
        print(f"✓ Subtask-Centric VL saved to: {new_file}")
        return

    # Rest of the original CLI code
    if args.question:
        if args.combined:
            print("\n" + "=" * 60)
            print("COMBINED MODE (LangGraph + Consensus)")
            print("=" * 60)
            print(f"Question: {args.question}")
            print("=" * 60)

            result = run_consensus_workflow(
                question=args.question,
                team_size=args.team_size,
                num_rounds=args.rounds,
                thread_id=f"cli_combined_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                verbose=args.verbose,
            )

            final_answer = result.get("final_answer", "No answer generated")

            print("\n" + "=" * 60)
            print("FINAL ANSWER:")
            print("=" * 60)
            print(final_answer)

            output_file = save_answer_to_file(
                final_answer, args.question, args.output, mode="combined"
            )
            print(f"\n✓ Answer saved to: {output_file}")

        elif args.langgraph:
            print("\n" + "=" * 60)
            print("LANGGRAPH MODE")
            print("=" * 60)
            print(f"Question: {args.question}")
            print("=" * 60)

            result = run_research_workflow(
                question=args.question,
                enable_human_review=False,
                thread_id=f"cli_langgraph_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                verbose=args.verbose,
            )

            final_answer = result.get("final_answer", "No answer generated")

            print("\n" + "=" * 60)
            print("FINAL ANSWER:")
            print("=" * 60)
            print(final_answer)

            output_file = save_answer_to_file(
                final_answer, args.question, args.output, mode="langgraph"
            )
            print(f"\n✓ Answer saved to: {output_file}")

        elif args.virtual_lab:
            print("\n" + "=" * 60)
            print("ORIGINAL VIRTUAL LAB MODE (Parallel)")
            print("=" * 60)
            print(f"Question: {args.question}")
            print(
                f"Configuration: {args.rounds} rounds, max {args.team_size} specialists"
            )
            print("=" * 60)

            final_answer = run_virtual_lab(
                question=args.question,
                api_key=args.api_key,
                model=args.model,
                provider=provider,
                num_rounds=args.rounds,
                max_team_size=args.team_size,
                verbose=args.verbose,
                data_dir=args.data_dir,
                input_dir=args.input_dir,
            )

            print("\n" + "=" * 60)
            print("FINAL ANSWER (PI Synthesis):")
            print("=" * 60)
            print(final_answer)

            output_file = save_answer_to_file(
                final_answer, args.question, args.output, mode="virtual-lab"
            )
            print(f"\n✓ Answer saved to: {output_file}")

        else:
            # Single agent mode
            agent = create_agent(
                api_key=args.api_key,
                model=args.model,
                provider=provider,
                data_dir=args.data_dir,
                input_dir=args.input_dir,
            )

            if args.with_critic:
                initial, critique, final = agent.run_with_critic(
                    args.question, verbose=args.verbose
                )
                print("\n" + "=" * 60)
                print("FINAL REFINED ANSWER:")
                print("=" * 60)
                print(final)
                output_file = save_answer_to_file(
                    final, args.question, args.output, mode="with-critic"
                )
                print(f"\n✓ Answer saved to: {output_file}")
            else:
                response = agent.run(args.question, verbose=args.verbose)
                print("\n" + "=" * 60)
                print("Final Answer:")
                print("=" * 60)
                print(response)
                output_file = save_answer_to_file(
                    response, args.question, args.output, mode="single-agent"
                )
                print(f"\n✓ Answer saved to: {output_file}")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
