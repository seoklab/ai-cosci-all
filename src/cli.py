"""Command-line interface for the bioinformatics agent."""

import argparse
import os
import sys
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# Load .env early so environment variables (OPENROUTER_API_KEY, etc.) are
# available before importing modules that may read them during import-time.
load_dotenv()

# Fix LiteLLM callback limit issue (default 30 is too low for multi-agent systems)
os.environ["LITELLM_MAX_CALLBACKS"] = "100"

# Check if CUDA is actually functional (not just stub library) BEFORE any torch imports
# This must happen before SentenceTransformer or any deep learning library is imported
try:
    import torch

    if torch.cuda.is_available():
        try:
            torch.zeros(1).cuda()
        except Exception:
            # CUDA stub library detected - force CPU mode
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
    else:
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
except Exception:
    os.environ["CUDA_VISIBLE_DEVICES"] = ""

from src.agent.agent import create_agent
from src.agent.meeting import run_virtual_lab
from src.agent.meeting_refactored import run_virtual_lab as run_virtual_lab_subtask
from src.virtuallab_workflow.workflow import (
    run_consensus_workflow,
    run_research_workflow,
)
from src.utils.output_manager import get_output_manager
from src.utils.logger import init_logger
from src.evaluation.single_evaluator import evaluate_answer


def save_answer_to_file(
    answer: str,
    question: str,
    output_path: str = None,
    mode: str = "single",
    eval_score: float = None,
    eval_explanation: str = None,
) -> str:
    """Save the final answer with evaluation to a markdown file.

    Args:
        answer: The final answer text
        question: The original question
        output_path: Optional custom output path
        mode: 'single' or 'virtual-lab' for different formatting
        eval_score: Optional evaluation score (1-10)
        eval_explanation: Optional evaluation explanation

    Returns:
        Path to the saved file
    """
    from src.utils.output_manager import get_current_run_dir

    # Generate filename if not provided
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Use OUTPUT_DIR if available
        run_dir = get_current_run_dir()
        if run_dir:
            output_path = run_dir / f"answer_{timestamp}.md"
        else:
            output_path = f"answer_{timestamp}.md"

    # Ensure it's a Path object
    output_file = Path(output_path)

    # Create parent directories if needed
    output_file.parent.mkdir(parents=True, exist_ok=True)

    # Format the content
    content = f"""# CoScientist Research Report
**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Mode:** {mode.replace("-", " ").title()}

---

## Research Question

{question}

---

## Final Answer

{answer}

---
"""

    # Add evaluation section if available
    if eval_score is not None:
        content += f"""
## Auto-Evaluation

**Score:** {eval_score:.1f} / 10.0

**Evaluation Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

### Detailed Feedback

{eval_explanation}

---
"""

    content += "\n*Generated by CoScientist - AI Research Assistant*\n"

    # Write to file
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(content)

    return str(output_file.absolute())


def auto_evaluate_answer(answer: str, question: str, verbose: bool = False):
    """Automatically evaluate the final answer and return results.

    Args:
        answer: The final answer to evaluate
        question: The original research question
        verbose: Print detailed evaluation

    Returns:
        Tuple of (score, explanation) or (None, None) if evaluation fails
    """
    try:
        print("\n" + "=" * 60)
        print("AUTO-EVALUATION (FastChat LLM-as-a-Judge)")
        print("=" * 60)

        # Use Claude 3.5 Sonnet as default judge
        eval_model = "anthropic/claude-3.5-sonnet"

        # Evaluate the answer
        eval_result = evaluate_answer(
            question=question, answer=answer, judge_model=eval_model, verbose=verbose
        )

        # Display score prominently
        print("\n" + "=" * 60)
        print(f"ðŸ“Š FINAL SCORE: {eval_result.score:.1f}/10.0")
        print("=" * 60)

        if not verbose:
            # Show brief explanation if not in verbose mode
            explanation_preview = (
                eval_result.explanation[:300] + "..."
                if len(eval_result.explanation) > 300
                else eval_result.explanation
            )
            print(f"\n{explanation_preview}")

        return eval_result.score, eval_result.explanation

    except Exception as e:
        print(f"\nâš ï¸  Auto-evaluation failed: {e}")
        print("Continuing without evaluation...")
        return None, None


def main():
    """Main CLI entry point."""

    parser = argparse.ArgumentParser(
        description="CoScientist: AI Research Assistant for Biomedical Questions",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Ask a single question
  python -m src.cli --question "How can gene signatures guide drug repositioning?"

  # Virtual Lab mode (multi-agent collaboration)
  python -m src.cli --question "..." --virtual-lab

  # Virtual Lab with more rounds and specialists
  python -m src.cli --question "..." --virtual-lab --rounds 3 --team-size 4

  # With critic feedback loop for quality validation (single agent)
  python -m src.cli --question "..." --with-critic

  # Interactive mode
  python -m src.cli --interactive

  # Save answer to a specific file
  python -m src.cli --question "..." --output results/my_answer.md

  # Use a different model
  python -m src.cli --question "..." --model "openai/gpt-4"

  # Use custom directories for databases and input data
  python -m src.cli --question "..." --data-dir "/path/to/databases" --input-dir "/path/to/Q5"

  # Verbose output to see tool calls
  python -m src.cli --question "..." --verbose
        """,
    )

    parser.add_argument(
        "--question",
        "-q",
        type=str,
        help="Question to ask the agent",
    )
    parser.add_argument(
        "--interactive",
        "-i",
        action="store_true",
        help="Run in interactive mode",
    )
    parser.add_argument(
        "--model",
        "-m",
        type=str,
        default=os.getenv("OPENROUTER_MODEL", "anthropic/claude-sonnet-4"),
        help="Model to use (defaults to OPENROUTER_MODEL env var or claude-sonnet-4)",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Print verbose output with tool calls",
    )
    parser.add_argument(
        "--with-critic",
        "-c",
        action="store_true",
        help="Enable critic feedback loop for quality validation (single agent mode)",
    )
    parser.add_argument(
        "--virtual-lab",
        "-vl",
        action="store_true",
        help="Enable Virtual Lab mode (multi-agent collaboration)",
    )
    parser.add_argument(
        "--subtask-centric",
        "-sc",
        action="store_true",
        help="Enable Subtask-Centric Virtual Lab mode (sequential, research plan-driven)",
    )
    parser.add_argument(
        "--save-intermediate",
        action="store_true",
        help="Save intermediate subtask results and round summaries to markdown files (only for subtask-centric mode)",
    )
    parser.add_argument(
        "--combined",
        action="store_true",
        help="Enable Combined Mode (LangGraph + Consensus)",
    )
    parser.add_argument(
        "--langgraph",
        action="store_true",
        help="Enable LangGraph workflow (without consensus)",
    )
    parser.add_argument(
        "--rounds",
        "-r",
        type=int,
        default=2,
        help="Number of discussion rounds in Virtual Lab mode (default: 2)",
    )
    parser.add_argument(
        "--team-size",
        "-t",
        type=int,
        default=3,
        help="Maximum number of specialist agents in Virtual Lab mode (default: 3)",
    )
    parser.add_argument(
        "--max-iterations",
        type=int,
        default=30,
        help="Maximum iterations per agent for tool use (default: 30, useful for testing with lower values)",
    )
    parser.add_argument(
        "--api-key",
        type=str,
        help="API key (Anthropic or OpenRouter, or set env var)",
    )
    parser.add_argument(
        "--data-dir",
        "-d",
        type=str,
        default=os.getenv(
            "DATABASE_DIR", "/home.galaxy4/sumin/project/aisci/Competition_Data"
        ),
        help="Path to main database directory (Drug databases, PPI, GWAS, etc.). Defaults to DATABASE_DIR env var",
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        default=os.getenv("INPUT_DIR"),
        help="Path to question-specific input data (e.g., gene signatures, expression data). Defaults to INPUT_DIR env var, or --data-dir if not set",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        help="Save the final answer to a file (supports .md, .txt). Auto-generates filename if not specified.",
    )
    parser.add_argument(
        "--s2-keywords",
        action="store_true",
        default=True,
        help="Use keyword extraction for Semantic Scholar (default: True)",
    )
    parser.add_argument(
        "--no-s2-keywords",
        dest="s2_keywords",
        action="store_false",
        help="Disable keyword extraction, use full question for Semantic Scholar",
    )

    args = parser.parse_args()

    # Set default input directory if not specified
    if args.input_dir is None:
        args.input_dir = args.data_dir

    # Set s2_use_keywords in config
    from src.config import get_global_config

    config = get_global_config()
    config.s2_use_keywords = args.s2_keywords

    # ===================================================================
    # CREATE RUN-SPECIFIC CACHE DIRECTORY FOR PAPERQA
    # This ensures all search_literature calls in this CLI execution
    # share the same PDF pool
    # ===================================================================
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Create safe directory name from question
    if args.question:
        # Clean question: only alphanumeric, spaces, hyphens
        clean_question = "".join(
            c for c in args.question if c.isalnum() or c in (" ", "-")
        ).strip()
        clean_question = clean_question.replace(" ", "_")
        # Limit length
        if len(clean_question) > 50:
            clean_question = clean_question[:50]
        dir_name = f"run_{timestamp}_{clean_question}"
    else:
        dir_name = f"run_{timestamp}_interactive"

    cache_base_dir = Path.cwd() / ".paperqa_cache"
    run_cache_dir = cache_base_dir / dir_name
    run_cache_dir.mkdir(parents=True, exist_ok=True)

    # Set environment variable for search_literature to use
    os.environ["PAPERQA_RUN_CACHE_DIR"] = str(run_cache_dir)
    print(f"âœ“ PaperQA cache directory: {run_cache_dir}")

    # ===================================================================
    # COPY PRE-PROVIDED PAPERS TO online_papers DIRECTORY
    # If user has papers in papers/ directory, copy them to cache
    # so they're available for search_literature from the start
    # ===================================================================
    import shutil
    from src.config import get_default_config

    config = get_default_config()
    papers_source_dir = Path(config.paper_library_dir)
    online_papers_dir = run_cache_dir / "online_papers"
    online_papers_dir.mkdir(exist_ok=True)

    if papers_source_dir.exists():
        pdf_files = list(papers_source_dir.glob("**/*.pdf"))
        if pdf_files:
            print(f"âœ“ Copying {len(pdf_files)} pre-provided papers to cache...")
            copied_count = 0
            for pdf_file in pdf_files:
                try:
                    dest_file = online_papers_dir / pdf_file.name
                    # Only copy if destination doesn't exist (avoid overwriting)
                    if not dest_file.exists():
                        shutil.copy2(pdf_file, dest_file)
                        copied_count += 1
                except Exception as e:
                    print(f"  Warning: Could not copy {pdf_file.name}: {e}")
            print(f"  â†’ {copied_count} papers copied to {online_papers_dir}")

    # Save run metadata
    run_metadata = {
        "question": args.question if args.question else "interactive",
        "mode": "subtask-centric"
        if args.subtask_centric
        else "virtual-lab"
        if args.virtual_lab
        else "single",
        "timestamp": timestamp,
        "model": args.model,
    }
    import json

    with open(run_cache_dir / "run_metadata.json", "w") as f:
        json.dump(run_metadata, f, indent=2)

    # Check for conflicting modes
    if args.virtual_lab and args.with_critic:
        print(
            "Error: Cannot use both --virtual-lab and --with-critic at the same time.",
            file=sys.stderr,
        )
        print(
            "Choose one mode: Virtual Lab (multi-agent) OR single agent with critic.",
            file=sys.stderr,
        )
        sys.exit(1)

    # Determine provider from model name or environment
    provider = None
    if args.model and "/" in args.model:
        # Model format like "anthropic/..." or "openai/..." indicates OpenRouter
        provider = "openrouter"
    else:
        provider = "anthropic"

    # Only create agent for non-virtual-lab modes
    if not args.virtual_lab:
        try:
            agent = create_agent(
                api_key=args.api_key,
                model=args.model,
                provider=provider,
                data_dir=args.data_dir,
                input_dir=args.input_dir,
            )
        except ValueError as e:
            print(f"Error: {e}", file=sys.stderr)
            sys.exit(1)

    if args.question:
        # Single question mode
        if args.combined:
            # Combined Mode (LangGraph + Consensus)
            print("\n" + "=" * 60)
            print("COMBINED MODE (LangGraph + Consensus)")
            print("=" * 60)
            print(f"Question: {args.question}")
            print("=" * 60)

            result = run_consensus_workflow(
                question=args.question,
                team_size=args.team_size,
                num_rounds=args.rounds,
                thread_id=f"cli_combined_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                verbose=args.verbose,
            )

            final_answer = result.get("final_answer", "No answer generated")

            print("\n" + "=" * 60)
            print("FINAL ANSWER:")
            print("=" * 60)
            print(final_answer)

            # Auto-evaluate
            eval_score, eval_explanation = auto_evaluate_answer(
                final_answer, args.question, args.verbose
            )

            # Save answer with evaluation
            output_file = save_answer_to_file(
                final_answer,
                args.question,
                args.output,
                mode="combined",
                eval_score=eval_score,
                eval_explanation=eval_explanation,
            )
            print(f"\nâœ“ Answer with evaluation saved to: {output_file}")

        elif args.langgraph:
            # LangGraph Mode (Standard)
            print("\n" + "=" * 60)
            print("LANGGRAPH MODE")
            print("=" * 60)
            print(f"Question: {args.question}")
            print("=" * 60)

            result = run_research_workflow(
                question=args.question,
                enable_human_review=False,
                thread_id=f"cli_langgraph_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                verbose=args.verbose,
            )

            final_answer = result.get("final_answer", "No answer generated")

            print("\n" + "=" * 60)
            print("FINAL ANSWER:")
            print("=" * 60)
            print(final_answer)

            # Auto-evaluate
            eval_score, eval_explanation = auto_evaluate_answer(
                final_answer, args.question, args.verbose
            )

            # Save answer with evaluation
            output_file = save_answer_to_file(
                final_answer,
                args.question,
                args.output,
                mode="langgraph",
                eval_score=eval_score,
                eval_explanation=eval_explanation,
            )
            print(f"\nâœ“ Answer with evaluation saved to: {output_file}")

        elif args.subtask_centric:
            # Initialize logger and start timer
            logger = init_logger(verbose=args.verbose, use_colors=True)
            logger.start_timer()

            # Subtask-Centric Virtual Lab mode - sequential research plan-driven collaboration
            logger.section("SUBTASK-CENTRIC VIRTUAL LAB MODE")
            logger.info(f"Question: {args.question}")
            logger.info(
                f"Configuration: {args.rounds} rounds, max {args.team_size} specialists"
            )
            logger.info(f"Model: {args.model}")
            print()

            # Create run-specific output directory
            output_mgr = get_output_manager()
            run_dir = output_mgr.create_run_directory(
                args.question, mode="subtask-centric"
            )
            logger.success(f"Output directory: {run_dir}")

            logger.progress("Starting Virtual Lab session...")
            print()

            final_answer = run_virtual_lab_subtask(
                question=args.question,
                api_key=args.api_key,
                model=args.model,
                provider=provider,
                num_rounds=args.rounds,
                max_team_size=args.team_size,
                verbose=args.verbose,
                data_dir=args.data_dir,
                input_dir=args.input_dir,
                max_iterations=args.max_iterations,
                save_intermediate=args.save_intermediate,
            )

            logger.section("FINAL SUBMISSION ANSWER")
            logger.info("(Clean version without internal review commentary)", indent=2)
            if args.save_intermediate:
                logger.info("(Internal version with red flag resolution saved separately)", indent=2)
            print()
            print(final_answer)

            # Auto-evaluate
            eval_score, eval_explanation = auto_evaluate_answer(
                final_answer, args.question, args.verbose
            )

            # Save answer with evaluation
            output_file = save_answer_to_file(
                final_answer,
                args.question,
                args.output,
                mode="subtask-centric",
                eval_score=eval_score,
                eval_explanation=eval_explanation,
            )
            logger.success(f"Answer with evaluation saved to: {output_file}")

            # Print total elapsed time
            logger.print_elapsed_time()

        elif args.subtask_centric:
            # Note: This block seems to be unreachable due to earlier subtask_centric handling
            # Keeping for compatibility, but consider removing in future refactor
            print("\n" + "=" * 60)
            print("SUBTASK-CENTRIC VIRTUAL LAB MODE")
            print("=" * 60)
            print(f"Question: {args.question}")
            print(
                f"Configuration: {args.rounds} rounds, max {args.team_size} specialists"
            )
            print("=" * 60)

            # Create run-specific output directory
            output_mgr = get_output_manager()
            run_dir = output_mgr.create_run_directory(
                args.question, mode="subtask-centric"
            )
            print(f"Output directory: {run_dir}\n")

            final_answer = run_virtual_lab_subtask(
                question=args.question,
                api_key=args.api_key,
                model=args.model,
                provider=provider,
                num_rounds=args.rounds,
                max_team_size=args.team_size,
                verbose=args.verbose,
                data_dir=args.data_dir,
                input_dir=args.input_dir,
                save_intermediate=args.save_intermediate,
            )

            print("\n" + "=" * 60)
            print("FINAL SUBMISSION ANSWER:")
            print("=" * 60)
            if args.save_intermediate:
                print("(Internal version with red flag resolution saved separately)\n")
            print(final_answer)

            # Save to file
            output_file = save_answer_to_file(
                final_answer, args.question, args.output, mode="subtask-centric"
            )
            print(f"\nâœ“ Answer saved to: {output_file}")

        elif args.virtual_lab:
            # Virtual Lab mode - multi-agent collaboration (original parallel model)
            print("\n" + "=" * 60)
            print("VIRTUAL LAB MODE")
            print("=" * 60)
            print(f"Question: {args.question}")
            print(
                f"Configuration: {args.rounds} rounds, max {args.team_size} specialists"
            )
            print("=" * 60)

            # Create run-specific output directory
            output_mgr = get_output_manager()
            run_dir = output_mgr.create_run_directory(args.question, mode="virtual-lab")
            print(f"Output directory: {run_dir}\n")

            final_answer = run_virtual_lab(
                question=args.question,
                api_key=args.api_key,
                model=args.model,
                provider=provider,
                num_rounds=args.rounds,
                max_team_size=args.team_size,
                verbose=args.verbose,
                data_dir=args.data_dir,
                input_dir=args.input_dir,
            )

            print("\n" + "=" * 60)
            print("FINAL ANSWER (PI Synthesis):")
            print("=" * 60)
            print(final_answer)

            # Auto-evaluate
            eval_score, eval_explanation = auto_evaluate_answer(
                final_answer, args.question, args.verbose
            )

            # Save answer with evaluation
            output_file = save_answer_to_file(
                final_answer,
                args.question,
                args.output,
                mode="virtual-lab",
                eval_score=eval_score,
                eval_explanation=eval_explanation,
            )
            print(f"\nâœ“ Answer with evaluation saved to: {output_file}")

        elif args.with_critic:
            initial, critique, final = agent.run_with_critic(
                args.question, verbose=args.verbose
            )
            print("\n" + "=" * 60)
            print("INITIAL ANSWER:")
            print("=" * 60)
            print(initial)
            print("\n" + "=" * 60)
            print("CRITIC FEEDBACK:")
            print("=" * 60)
            print(critique)
            print("\n" + "=" * 60)
            print("FINAL REFINED ANSWER:")
            print("=" * 60)
            print(final)

            # Auto-evaluate the final refined answer
            eval_score, eval_explanation = auto_evaluate_answer(
                final, args.question, args.verbose
            )

            # Save answer with evaluation (save the final refined answer)
            output_file = save_answer_to_file(
                final,
                args.question,
                args.output,
                mode="with-critic",
                eval_score=eval_score,
                eval_explanation=eval_explanation,
            )
            print(f"\nâœ“ Answer with evaluation saved to: {output_file}")
        else:
            response = agent.run(args.question, verbose=args.verbose)
            print("\n" + "=" * 60)
            print("Final Answer:")
            print("=" * 60)
            print(response)

            # Auto-evaluate
            eval_score, eval_explanation = auto_evaluate_answer(
                response, args.question, args.verbose
            )

            # Save answer with evaluation
            output_file = save_answer_to_file(
                response,
                args.question,
                args.output,
                mode="single-agent",
                eval_score=eval_score,
                eval_explanation=eval_explanation,
            )
            print(f"\nâœ“ Answer with evaluation saved to: {output_file}")

    elif args.interactive:
        # Interactive mode
        print("=" * 60)
        print("CoScientist: Interactive Mode")
        if args.subtask_centric:
            print("(Subtask-Centric Virtual Lab - Research Plan-Driven)")
        elif args.virtual_lab:
            print("(Virtual Lab - Multi-Agent Collaboration)")
        print("=" * 60)
        print("Ask biomedical research questions. Type 'exit' or 'quit' to exit.\n")

        while True:
            try:
                question = input("Question: ").strip()
            except EOFError:
                break

            if question.lower() in ["exit", "quit"]:
                print("Goodbye!")
                break

            if not question:
                continue

            if args.subtask_centric:
                # Initialize logger for this question and start timer
                logger = init_logger(verbose=args.verbose, use_colors=True)
                logger.start_timer()

                # Subtask-Centric Virtual Lab mode in interactive
                logger.section("SUBTASK-CENTRIC VIRTUAL LAB MEETING")
                logger.info(f"Question: {question}")

                # Create run-specific output directory for this question
                output_mgr = get_output_manager()
                run_dir = output_mgr.create_run_directory(
                    question, mode="subtask-centric"
                )
                logger.success(f"Output directory: {run_dir}")
                print()
                print(f"Output directory: {run_dir}\n")

                final_answer = run_virtual_lab_subtask(
                    question=question,
                    api_key=args.api_key,
                    model=args.model,
                    provider=provider,
                    num_rounds=args.rounds,
                    max_team_size=args.team_size,
                    verbose=args.verbose,
                    data_dir=args.data_dir,
                    input_dir=args.input_dir,
                    max_iterations=args.max_iterations,
                    save_intermediate=args.save_intermediate,
                )

                logger.section("FINAL SUBMISSION ANSWER")
                if args.save_intermediate:
                    logger.info("(Internal version with red flag resolution saved separately)", indent=2)

                print("\n" + "=" * 60)
                print("FINAL SUBMISSION ANSWER:")
                print("=" * 60)
                if args.save_intermediate:
                    print("(Internal version with red flag resolution saved separately)\n")
                print(final_answer)

                # Auto-save in interactive mode
                output_file = save_answer_to_file(
                    final_answer, question, mode="subtask-centric"
                )
                logger.success(f"Saved to: {output_file}")

                # Print elapsed time for this question
                logger.print_elapsed_time()
                print(f"âœ“ Saved to: {output_file}")
                print()

            elif args.virtual_lab:
                # Virtual Lab mode in interactive
                print("\n" + "=" * 60)
                print("VIRTUAL LAB MEETING")
                print("=" * 60)

                # Create run-specific output directory
                output_mgr = get_output_manager()
                run_dir = output_mgr.create_run_directory(question, mode="virtual-lab")
                print(f"Output directory: {run_dir}\n")

                final_answer = run_virtual_lab(
                    question=question,
                    api_key=args.api_key,
                    model=args.model,
                    provider=provider,
                    num_rounds=args.rounds,
                    max_team_size=args.team_size,
                    verbose=args.verbose,
                    data_dir=args.data_dir,
                    input_dir=args.input_dir,
                    max_iterations=args.max_iterations,
                )

                print("\n" + "=" * 60)
                print("FINAL ANSWER:")
                print("=" * 60)
                print(final_answer)

                # Auto-save in interactive mode with timestamp
                output_file = save_answer_to_file(
                    final_answer, question, mode="virtual-lab"
                )
                print(f"âœ“ Saved to: {output_file}")
                print()

            elif args.with_critic:
                initial, critique, final = agent.run_with_critic(
                    question, verbose=args.verbose
                )
                print("\n" + "=" * 60)
                print("INITIAL ANSWER:")
                print("=" * 60)
                print(initial)
                print("\n" + "=" * 60)
                print("CRITIC FEEDBACK:")
                print("=" * 60)
                print(critique)
                print("\n" + "=" * 60)
                print("FINAL REFINED ANSWER:")
                print("=" * 60)
                print(final)

                # Auto-save in interactive mode
                output_file = save_answer_to_file(final, question, mode="with-critic")
                print(f"âœ“ Saved to: {output_file}")
                print()
            else:
                response = agent.run(question, verbose=args.verbose)
                print("\n" + "=" * 60)
                print("Answer:")
                print("=" * 60)
                print(response)

                # Auto-save in interactive mode
                output_file = save_answer_to_file(
                    response, question, mode="single-agent"
                )
                print(f"âœ“ Saved to: {output_file}")
                print()

    else:
        # No input provided
        parser.print_help()


if __name__ == "__main__":
    main()
