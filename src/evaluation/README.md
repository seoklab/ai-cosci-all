# AI-CoSci Pairwise Evaluation Guide

Guide for using the pairwise comparison system to evaluate answer quality.

---

## Installation & Setup

### Install Required Packages

```bash
pip install requests python-dotenv
```

### Configure API Key

Ensure your OpenRouter API key is set in the `.env` file:

```bash
OPENROUTER_API_KEY=sk-or-v1-xxxxx
```

---

## Basic Usage

### 1. Compare Two Answer Files

```bash
python src/evaluation/pairwise_evaluator.py \
  --question "What are effective approaches for identifying drug targets in cancer therapy?" \
  --answer-a examples/answer_a_cancer.md \
  --answer-b examples/answer_b_cancer.md \
  --verbose
```

**Output:**
- Evaluation result saved to `tests/evaluation/pairwise_result_YYYYMMDD_HHMMSS.md`
- Winner and detailed analysis printed to terminal

### 2. Load Question from File

```bash
# Save question to file
echo "Explain the mechanisms of CRISPR-Cas9 gene editing" > question.txt

python src/evaluation/pairwise_evaluator.py \
  -q question.txt \
  -a answer1.md \
  -b answer2.md
```

### 3. Specify Custom Output Path

```bash
python src/evaluation/pairwise_evaluator.py \
  -q "Gene therapy approaches for inherited diseases" \
  -a results/model_a_answer.txt \
  -b results/model_b_answer.txt \
  --output experiments/comparison_20251219.md
```

### 4. Use Different Judge Model

```bash
python src/evaluation/pairwise_evaluator.py \
  -q "Molecular mechanisms of Alzheimer's disease" \
  -a answer_claude.md \
  -b answer_gpt.md \
  --judge-model "openai/gpt-4" \
  --verbose
```

---

## Practical Examples

### Scenario 1: Compare Answers from Different Models

Compare answers generated by different LLMs:

```bash
# Claude vs GPT-4
python src/evaluation/pairwise_evaluator.py \
  -q "What are the key challenges in developing mRNA vaccines?" \
  -a outputs/claude_sonnet_answer.md \
  -b outputs/gpt4_answer.md \
  -v

# Gemini vs Claude
python src/evaluation/pairwise_evaluator.py \
  -q "What are the key challenges in developing mRNA vaccines?" \
  -a outputs/gemini_answer.md \
  -b outputs/claude_sonnet_answer.md \
  -v
```

### Scenario 2: Virtual Lab vs Single Agent Comparison

Compare different approaches to the same question:

```bash
python src/evaluation/pairwise_evaluator.py \
  -q problems/ex1.txt \
  -a tests/q1_virtuallab_answer.md \
  -b tests/q1_single_agent_answer.md \
  --output tests/evaluation/virtuallab_vs_single.md
```

### Scenario 3: Batch Evaluation (Multiple Questions)

Evaluate answers for multiple questions sequentially:

```bash
#!/bin/bash
# batch_evaluate.sh

QUESTIONS=("problems/ex1.txt" "problems/ex2.txt" "problems/ex3.txt")

for q in "${QUESTIONS[@]}"; do
  qname=$(basename "$q" .txt)
  python src/evaluation/pairwise_evaluator.py \
    -q "$q" \
    -a "answers/model_a_${qname}.md" \
    -b "answers/model_b_${qname}.md" \
    --output "tests/evaluation/comparison_${qname}.md"
done
```

---

## Evaluation Criteria

The judge model evaluates answers based on:

1. **Scientific Accuracy**: Correctness of scientific facts and interpretations
2. **Evidence Quality**: PMID citations, reliability of sources
3. **Methodological Rigor**: Appropriateness of analysis methods
4. **Completeness**: Thoroughness in addressing the question
5. **Clarity**: Clarity of scientific explanations
6. **Limitations**: Acknowledgment of uncertainties and limitations
7. **Relevance**: Direct relevance to the research question

---

## Output File Format

Evaluation results are saved as Markdown files with the following structure:

```markdown
# Pairwise Evaluation Result

**Evaluation Date:** 2025-12-19 16:30:45

---

## Research Question

What are effective approaches for identifying drug targets in cancer therapy?

---

## Answer Files Compared

- **Answer A:** `examples/answer_a_cancer.md`
- **Answer B:** `examples/answer_b_cancer.md`

---

## Evaluation Result

### Winner: A

### Detailed Analysis

[Detailed analysis from the judge...]

---

## Metadata

- **Evaluation Timestamp:** 2025-12-19T16:30:45
- **Score A:** N/A
- **Score B:** N/A
```

---

## Advanced Options

### Available Judge Models

- `anthropic/claude-3.5-sonnet` (default, recommended)
- `anthropic/claude-sonnet-4`
- `openai/gpt-4`
- `openai/gpt-4-turbo`
- `google/gemini-pro`

### Full Options List

```bash
python src/evaluation/pairwise_evaluator.py --help
```

**Key Options:**
- `-q, --question`: Research question (string or file path)
- `-a, --answer-a`: First answer file
- `-b, --answer-b`: Second answer file
- `-o, --output`: Output file path (default: auto-generated)
- `--judge-model`: Model to use as judge
- `-v, --verbose`: Verbose output

---

## Tips & Best Practices

### ✅ Do's

- **Use Consistent Questions**: Compare answers to the same question
- **Provide Sufficient Context**: Include necessary background information in the question
- **Test Multiple Judges**: Evaluate with different judge models to verify consistency
- **Document Results**: Record evaluation results in project documentation

### ❌ Don'ts

- **Don't Compare Different Questions**: Avoid comparing answers to different questions
- **Don't Use Too Short Answers**: Recommend at least 2-3 paragraphs
- **Don't Expose API Keys**: Never put API keys directly in command line (use `.env`)

---

## Troubleshooting

### API Key Error

```
ValueError: OPENROUTER_API_KEY not found in environment
```

**Solution:** Add API key to `.env` file or set environment variable

```bash
export OPENROUTER_API_KEY=sk-or-v1-xxxxx
```

### File Not Found

```
FileNotFoundError: File not found: answer.md
```

**Solution:** Use absolute path or relative path from project root

### Slow Evaluation Speed

**Solution:** Use a faster judge model

```bash
--judge-model "anthropic/claude-3-haiku"
```

---

## Real Usage Examples

### Example 1: Evaluate Q1 Problem

```bash
python src/evaluation/pairwise_evaluator.py \
  --question "Identify differentially expressed genes in CD4+ T cell subtypes" \
  --answer-a tests/q1_answer_v1.md \
  --answer-b tests/q1_answer_v2.md \
  --output tests/evaluation/q1_comparison.md \
  --verbose
```

### Example 2: Cancer Research Comparison

```bash
python src/evaluation/pairwise_evaluator.py \
  -q examples/cancer_question.txt \
  -a examples/answer_a_cancer.md \
  -b examples/answer_b_cancer.md \
  --judge-model "anthropic/claude-3.5-sonnet" \
  -v
```

### Example 3: Evaluate Competition Answers

```bash
# Q1: T cell analysis
python src/evaluation/pairwise_evaluator.py \
  -q "Analyze differential gene expression in CD4+ T cell subtypes" \
  -a tests/q1_method1.md \
  -b tests/q1_method2.md \
  -o tests/evaluation/q1_eval.md

# Q5: Time series DEG analysis
python src/evaluation/pairwise_evaluator.py \
  -q "Identify temporal patterns in gene expression during infection" \
  -a tests/q5_approach1.md \
  -b tests/q5_approach2.md \
  -o tests/evaluation/q5_eval.md
```

---

## Quick Reference Commands

```bash
# Basic comparison
python src/evaluation/pairwise_evaluator.py -q "your question" -a file1.md -b file2.md -v

# With custom output
python src/evaluation/pairwise_evaluator.py -q question.txt -a ans1.md -b ans2.md -o results/eval.md

# Different judge model
python src/evaluation/pairwise_evaluator.py -q "question" -a ans1.md -b ans2.md --judge-model "openai/gpt-4"

# Quiet mode (no verbose)
python src/evaluation/pairwise_evaluator.py -q "question" -a ans1.md -b ans2.md
```

---

## References

- [FastChat LLM Judge Paper](https://arxiv.org/abs/2306.05685)
- [OpenRouter API Documentation](https://openrouter.ai/docs)
- [AI-CoSci Project Documentation](../README.md)

---

**Last Updated:** 2025-12-19
