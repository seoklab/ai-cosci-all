# AI-CoSci Evaluation System

Complete guide for automatic answer evaluation using LLM-as-a-judge (FastChat approach).

---

## Overview

The evaluation system provides two main modes:

1. **Auto-Evaluation**: Automatically scores every generated answer (1-10 scale)
   - Runs automatically for all CLI modes
   - Integrated into answer files
   - Based on FastChat single-v1 judging approach

2. **Pairwise Comparison**: Compare two answers side-by-side
   - Determines winner (A, B, or Tie)
   - Detailed comparative analysis
   - Saved to `tests/evaluation/` directory

---

## Installation & Setup

### Install Required Packages

```bash
pip install requests python-dotenv
```

### Configure API Key

Ensure your OpenRouter API key is set in the `.env` file:

```bash
OPENROUTER_API_KEY=sk-or-v1-xxxxx
```

---

## Auto-Evaluation System

### How It Works

**Every answer is automatically evaluated** - no flags or setup needed!

#### Evaluation Flow

1. Agent generates answer to your question
2. Auto-evaluator analyzes answer quality (1-10 scale)
3. Evaluation results integrated into answer file
4. Score and feedback displayed in terminal

#### Evaluation Criteria

Answers are scored based on:

- **Scientific Accuracy** (30%): Correctness and validity of scientific content
- **Evidence Quality** (20%): Citations, references, and supporting evidence
- **Methodological Rigor** (15%): Appropriateness of analytical approaches
- **Completeness** (15%): Coverage of question aspects
- **Clarity** (10%): Organization and readability
- **Critical Thinking** (10%): Depth of analysis and insights

#### Example Output

```bash
python -m src.cli --question "What is T cell exhaustion?" --verbose

# Terminal output:
============================================================
AUTO-EVALUATION (FastChat LLM-as-a-Judge)
============================================================

============================================================
üìä FINAL SCORE: 7.0/10.0
============================================================

Here's my expert evaluation of the response:

**Scientific Accuracy (30%): 25/30**
- Accurately describes T cell exhaustion as a distinct differentiation state
- Correctly identifies key molecular players (TOX, TCF-1)
...
```

#### Answer File Structure

Every answer file includes evaluation results:

```markdown
# CoScientist Research Report
**Generated:** 2025-12-21 00:55:07
**Mode:** Combined

## Research Question
What is T cell exhaustion?

## Final Answer
[Answer content...]

---

## Auto-Evaluation

**Score:** 7.0 / 10.0

**Evaluation Date:** 2025-12-21 00:55:19

### Detailed Feedback

Here's my expert evaluation of the response:

**Scientific Accuracy (30%): 25/30**
- Accurately describes T cell exhaustion...
[Full evaluation feedback...]
```

#### Configuration

**Default Judge Model:** `anthropic/claude-3.5-sonnet`

To change the judge model, edit `src/cli.py`:

```python
def auto_evaluate_answer(answer: str, question: str, verbose: bool = False):
    # Change this line:
    eval_model = "anthropic/claude-3.5-sonnet"
```

Recommended models:
- `anthropic/claude-3.5-sonnet` (default, best balance)
- `anthropic/claude-sonnet-4` (highest quality)
- `google/gemini-3-pro-preview` (fast, lower cost)

#### Files Location

- **Answer files**: Auto-generated `answer_YYYYMMDD_HHMMSS.md` in project root (or custom path)
- **Evaluation included**: Integrated in answer file (see "Auto-Evaluation" section)

---

## Pairwise Comparison

### Basic Usage

### 1. Compare Two Answer Files

```bash
python src/evaluation/pairwise_evaluator.py \
  --question "What are effective approaches for identifying drug targets in cancer therapy?" \
  --answer-a examples/answer_a_cancer.md \
  --answer-b examples/answer_b_cancer.md \
  --verbose
```

**Output:**
- Evaluation result saved to `tests/evaluation/pairwise_result_YYYYMMDD_HHMMSS.md`
- Winner and detailed analysis printed to terminal

### 2. Load Question from File

```bash
# Save question to file
echo "Explain the mechanisms of CRISPR-Cas9 gene editing" > question.txt

python src/evaluation/pairwise_evaluator.py \
  -q question.txt \
  -a answer1.md \
  -b answer2.md
```

### 3. Specify Custom Output Path

```bash
python src/evaluation/pairwise_evaluator.py \
  -q "Gene therapy approaches for inherited diseases" \
  -a results/model_a_answer.txt \
  -b results/model_b_answer.txt \
  --output experiments/comparison_20251219.md
```

### 4. Use Different Judge Model

```bash
python src/evaluation/pairwise_evaluator.py \
  -q "Molecular mechanisms of Alzheimer's disease" \
  -a answer_claude.md \
  -b answer_gpt.md \
  --judge-model "openai/gpt-4" \
  --verbose
```

---

## Practical Examples

### Scenario 1: Compare Answers from Different Models

Compare answers generated by different LLMs:

```bash
# Claude vs GPT-4
python src/evaluation/pairwise_evaluator.py \
  -q "What are the key challenges in developing mRNA vaccines?" \
  -a outputs/claude_sonnet_answer.md \
  -b outputs/gpt4_answer.md \
  -v

# Gemini vs Claude
python src/evaluation/pairwise_evaluator.py \
  -q "What are the key challenges in developing mRNA vaccines?" \
  -a outputs/gemini_answer.md \
  -b outputs/claude_sonnet_answer.md \
  -v
```

### Scenario 2: Virtual Lab vs Single Agent Comparison

Compare different approaches to the same question:

```bash
python src/evaluation/pairwise_evaluator.py \
  -q problems/ex1.txt \
  -a tests/q1_virtuallab_answer.md \
  -b tests/q1_single_agent_answer.md \
  --output tests/evaluation/virtuallab_vs_single.md
```

### Scenario 3: Batch Evaluation (Multiple Questions)

Evaluate answers for multiple questions sequentially:

```bash
#!/bin/bash
# batch_evaluate.sh

QUESTIONS=("problems/ex1.txt" "problems/ex2.txt" "problems/ex3.txt")

for q in "${QUESTIONS[@]}"; do
  qname=$(basename "$q" .txt)
  python src/evaluation/pairwise_evaluator.py \
    -q "$q" \
    -a "answers/model_a_${qname}.md" \
    -b "answers/model_b_${qname}.md" \
    --output "tests/evaluation/comparison_${qname}.md"
done
```

---

## Evaluation Criteria

### Auto-Evaluation Scoring

Each answer receives a score from 1-10 based on:

1. **Scientific Accuracy (30%)**: Correctness of facts, validity of interpretations
2. **Evidence Quality (20%)**: Quality and relevance of citations (PMIDs preferred)
3. **Methodological Rigor (15%)**: Appropriateness of analytical methods
4. **Completeness (15%)**: Thoroughness in addressing all question aspects
5. **Clarity (10%)**: Organization, readability, scientific communication
6. **Critical Thinking (10%)**: Analysis depth, insights, limitations

**Score Interpretation:**
- 9-10: Outstanding answer with comprehensive evidence
- 7-8: Strong answer, minor improvements possible
- 5-6: Adequate answer with notable gaps
- 3-4: Weak answer, significant issues
- 1-2: Poor answer, major scientific errors

### Pairwise Comparison

The judge model compares answers based on:

1. **Scientific Accuracy**: Correctness of scientific facts and interpretations
2. **Evidence Quality**: PMID citations, reliability of sources
3. **Methodological Rigor**: Appropriateness of analysis methods
4. **Completeness**: Thoroughness in addressing the question
5. **Clarity**: Clarity of scientific explanations
6. **Limitations**: Acknowledgment of uncertainties and limitations
7. **Relevance**: Direct relevance to the research question

---

## Output File Format

Evaluation results are saved as Markdown files with the following structure:

```markdown
# Pairwise Evaluation Result

**Evaluation Date:** 2025-12-19 16:30:45

---

## Research Question

What are effective approaches for identifying drug targets in cancer therapy?

---

## Answer Files Compared

- **Answer A:** `examples/answer_a_cancer.md`
- **Answer B:** `examples/answer_b_cancer.md`

---

## Evaluation Result

### Winner: A

### Detailed Analysis

[Detailed analysis from the judge...]

---

## Metadata

- **Evaluation Timestamp:** 2025-12-19T16:30:45
- **Score A:** N/A
- **Score B:** N/A
```

---

## Advanced Options

### Available Judge Models

- `anthropic/claude-3.5-sonnet` (default, recommended)
- `anthropic/claude-sonnet-4`
- `openai/gpt-4`
- `openai/gpt-4-turbo`
- `google/gemini-pro`

### Full Options List

```bash
python src/evaluation/pairwise_evaluator.py --help
```

**Key Options:**
- `-q, --question`: Research question (string or file path)
- `-a, --answer-a`: First answer file
- `-b, --answer-b`: Second answer file
- `-o, --output`: Output file path (default: auto-generated)
- `--judge-model`: Model to use as judge
- `-v, --verbose`: Verbose output

---

## Tips & Best Practices

### ‚úÖ Do's

- **Use Consistent Questions**: Compare answers to the same question
- **Provide Sufficient Context**: Include necessary background information in the question
- **Test Multiple Judges**: Evaluate with different judge models to verify consistency
- **Document Results**: Record evaluation results in project documentation

### ‚ùå Don'ts

- **Don't Compare Different Questions**: Avoid comparing answers to different questions
- **Don't Use Too Short Answers**: Recommend at least 2-3 paragraphs
- **Don't Expose API Keys**: Never put API keys directly in command line (use `.env`)

---

## Troubleshooting

### API Key Error

```
ValueError: OPENROUTER_API_KEY not found in environment
```

**Solution:** Add API key to `.env` file or set environment variable

```bash
export OPENROUTER_API_KEY=sk-or-v1-xxxxx
```

### File Not Found

```
FileNotFoundError: File not found: answer.md
```

**Solution:** Use absolute path or relative path from project root

### Slow Evaluation Speed

**Solution:** Use a faster judge model

```bash
--judge-model "anthropic/claude-3-haiku"
```

---

## Real Usage Examples

### Example 1: Evaluate Q1 Problem

```bash
python src/evaluation/pairwise_evaluator.py \
  --question "Identify differentially expressed genes in CD4+ T cell subtypes" \
  --answer-a tests/q1_answer_v1.md \
  --answer-b tests/q1_answer_v2.md \
  --output tests/evaluation/q1_comparison.md \
  --verbose
```

### Example 2: Cancer Research Comparison

```bash
python src/evaluation/pairwise_evaluator.py \
  -q examples/cancer_question.txt \
  -a examples/answer_a_cancer.md \
  -b examples/answer_b_cancer.md \
  --judge-model "anthropic/claude-3.5-sonnet" \
  -v
```

### Example 3: Evaluate Competition Answers

```bash
# Q1: T cell analysis
python src/evaluation/pairwise_evaluator.py \
  -q "Analyze differential gene expression in CD4+ T cell subtypes" \
  -a tests/q1_method1.md \
  -b tests/q1_method2.md \
  -o tests/evaluation/q1_eval.md

# Q5: Time series DEG analysis
python src/evaluation/pairwise_evaluator.py \
  -q "Identify temporal patterns in gene expression during infection" \
  -a tests/q5_approach1.md \
  -b tests/q5_approach2.md \
  -o tests/evaluation/q5_eval.md
```

---Implementation Details

### Auto-Evaluation Implementation

Located in `src/evaluation/single_evaluator.py`:

```python
from src.evaluation.single_evaluator import evaluate_answer

# Evaluate an answer
result = evaluate_answer(
    question="What is T cell exhaustion?",
    answer="[Generated answer...]",
    judge_model="anthropic/claude-3.5-sonnet",
    verbose=True
)

print(f"Score: {result.score}/10.0")
print(f"Feedback: {result.explanation}")
```

**Key Functions:**
- `evaluate_answer()`: Main entry point for single answer evaluation
- `SingleAnswerJudge`: OpenRouter-based judge class
- `create_biomedical_evaluation_prompt()`: Prompt engineering for biomedical content

### Pairwise Comparison Implementation

Located in `src/evaluation/pairwise_evaluator.py`:

```python
from src.evaluation.pairwise_evaluator import evaluate_pairwise

# Compare two answers
result = evaluate_pairwise(
    question="What is CRISPR?",
    answer_a="[Answer A content...]",
    answer_b="[Answer B content...]",
    judge_model="anthropic/claude-3.5-sonnet"
)

print(f"Winner: {result.winner}")  # "A", "B", or "Tie"
```

### Integration with CLI

All CLI modes automatically trigger evaluation:

```python
# src/cli.py
def main():
    # ... generate answer ...
    
    # Auto-evaluate (always runs)
    eval_score, eval_explanation = auto_evaluate_answer(
        answer=final_answer,
        question=args.question,
        verbose=args.verbose
    )
    
    # Save with integrated evaluation
    output_file = save_answer_to_file(
        answer=final_answer,
        question=args.question,
        eval_score=eval_score,
        eval_explanation=eval_explanation
    )
```

---

## API Usage

### Programmatic Evaluation

```python
from src.evaluation.single_evaluator import evaluate_answer
from src.evaluation.pairwise_evaluator import evaluate_pairwise

# Single answer evaluation
result = evaluate_answer(
    question="Your research question",
    answer="Generated answer",
    judge_model="anthropic/claude-3.5-sonnet",
    verbose=True
)

# Access results
print(f"Score: {result.score}")
print(f"Explanation: {result.explanation}")

# Pairwise comparison
comparison = evaluate_pairwise(
    question="Your research question",
    answer_a="First answer",
    answer_b="Second answer"
)

print(f"Winner: {comparison.winner}")
print(f"Analysis: {comparison.explanation}")
```

---

## Troubleshooting

### Evaluation Not Running

**Symptom:** No evaluation section in answer file

**Solutions:**
1. Check API key in `.env`: `OPENROUTER_API_KEY=sk-or-v1-...`
2. Verify network connection to OpenRouter API
3. Check for error messages in terminal output

### Low Scores

**Common reasons:**
- Missing citations/PMIDs
- Incomplete coverage of question aspects
- Lack of critical analysis or limitations discussion
- Scientific inaccuracies

**Improvements:**
- Add literature references with PMIDs
- Use `search_pubmed` tool for citations
- Include methodological details
- Discuss limitations and future directions

### API Rate Limits

**Symptom:** `429 Too Many Requests` error

**Solutions:**
- Wait a few seconds between evaluations
- Use lower-tier judge model for testing
- Consider batching evaluations

---

## References

- [FastChat LLM Judge Paper](https://arxiv.org/abs/2306.05685)
- [OpenRouter API Documentation](https://openrouter.ai/docs)
- [AI-CoSci Project Documentation](../../README.md)
- [Auto-Evaluation Implementation Details](../../docs/AUTO_EVAL_IMPLEMENTATION.md)

---

**Last Updated:** 2025-12-21
python src/evaluation/pairwise_evaluator.py -q question.txt -a ans1.md -b ans2.md -o results/eval.md

# Different judge model
python src/evaluation/pairwise_evaluator.py -q "question" -a ans1.md -b ans2.md --judge-model "openai/gpt-4"

# Quiet mode (no verbose)
python src/evaluation/pairwise_evaluator.py -q "question" -a ans1.md -b ans2.md
```

---

## References

- [FastChat LLM Judge Paper](https://arxiv.org/abs/2306.05685)
- [OpenRouter API Documentation](https://openrouter.ai/docs)
- [AI-CoSci Project Documentation](../README.md)

---

**Last Updated:** 2025-12-19
