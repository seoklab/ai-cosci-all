"""Generic evaluation framework for multi-model answer evaluation.

This module provides high-level evaluation interfaces that work with various
models and evaluation strategies (FastChat judge, internal critics, etc.).
"""

import json
from typing import Optional, Dict, List, Any
from dataclasses import dataclass, asdict
from enum import Enum

from src.agent.fastchat_critic import FastChatCritic, EvaluationResult, CriticScore


class EvaluationStrategy(Enum):
    """Available evaluation strategies."""
    FASTCHAT = "fastchat"  # Use FastChat LLM judge
    INTERNAL = "internal"  # Use internal critic (existing run_with_critic)
    HYBRID = "hybrid"      # Use both strategies


@dataclass
class ModelAnswer:
    """Wrapper for a model's answer."""
    model_name: str
    answer: str
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "model_name": self.model_name,
            "answer": self.answer,
            "metadata": self.metadata or {},
        }


@dataclass
class ComparisonMetrics:
    """Metrics for comparing multiple models."""
    question: str
    models_evaluated: List[str]
    average_score: float
    highest_scoring_model: str
    lowest_scoring_model: str
    score_variance: float
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return asdict(self)


class AnswerEvaluator:
    """High-level evaluator for answers from multiple models.
    
    This evaluator abstracts away the specific evaluation strategy and provides
    a simple interface for evaluating answers generated by various models.
    """
    
    def __init__(
        self,
        strategy: EvaluationStrategy = EvaluationStrategy.FASTCHAT,
        judge_model: str = "gpt-4",
        api_provider: str = "openai",
        api_key: Optional[str] = None,
    ):
        """Initialize the evaluator.
        
        Args:
            strategy: Evaluation strategy to use
            judge_model: Judge model (for FastChat strategy)
            api_provider: API provider
            api_key: API key for judge model
        """
        self.strategy = strategy
        
        # Initialize strategy-specific components
        if strategy in [EvaluationStrategy.FASTCHAT, EvaluationStrategy.HYBRID]:
            self.critic = FastChatCritic(
                judge_model=judge_model,
                api_provider=api_provider,
                api_key=api_key,
            )
        else:
            self.critic = None
    
    def evaluate_answer(
        self,
        question: str,
        answer: str,
        model_name: str = "unknown",
        categories: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Evaluate a single answer.
        
        Args:
            question: The original question
            answer: The answer to evaluate
            model_name: Name of the model that generated the answer
            categories: Evaluation categories (if None, use defaults)
            
        Returns:
            Evaluation results as dictionary
        """
        if self.strategy == EvaluationStrategy.FASTCHAT:
            result = self.critic.evaluate(question, answer, categories)
            return {
                "model_name": model_name,
                "overall_score": result.overall_score,
                "reasoning": result.overall_reasoning,
                "category_scores": {k: v.score for k, v in result.scores.items()},
                "detailed_scores": {k: v.to_dict() for k, v in result.scores.items()},
                "timestamp": result.timestamp,
            }
        else:
            raise NotImplementedError(f"Strategy {self.strategy} not yet implemented")
    
    def evaluate_multiple(
        self,
        question: str,
        answers: Dict[str, str],
        categories: Optional[List[str]] = None,
    ) -> Dict[str, Dict[str, Any]]:
        """Evaluate multiple answers for the same question.
        
        Args:
            question: The original question
            answers: Dictionary of model_name -> answer
            categories: Evaluation categories
            
        Returns:
            Dictionary of model_name -> evaluation results
        """
        results = {}
        for model_name, answer in answers.items():
            results[model_name] = self.evaluate_answer(
                question, answer, model_name, categories
            )
        return results
    
    def compute_comparison_metrics(
        self,
        question: str,
        evaluation_results: Dict[str, Dict[str, Any]],
    ) -> ComparisonMetrics:
        """Compute comparison metrics for multiple model evaluations.
        
        Args:
            question: The original question
            evaluation_results: Results from evaluate_multiple()
            
        Returns:
            ComparisonMetrics object
        """
        scores = [r["overall_score"] for r in evaluation_results.values()]
        models = list(evaluation_results.keys())
        
        if not scores:
            return ComparisonMetrics(
                question=question,
                models_evaluated=models,
                average_score=0.0,
                highest_scoring_model="N/A",
                lowest_scoring_model="N/A",
                score_variance=0.0,
            )
        
        # Calculate metrics
        avg_score = sum(scores) / len(scores)
        highest_model = max(evaluation_results.items(), key=lambda x: x[1]["overall_score"])
        lowest_model = min(evaluation_results.items(), key=lambda x: x[1]["overall_score"])
        
        # Variance
        variance = sum((s - avg_score) ** 2 for s in scores) / len(scores)
        
        return ComparisonMetrics(
            question=question,
            models_evaluated=models,
            average_score=avg_score,
            highest_scoring_model=highest_model[0],
            lowest_scoring_model=lowest_model[0],
            score_variance=variance,
        )
    
    def generate_report(
        self,
        question: str,
        evaluation_results: Dict[str, Dict[str, Any]],
        comparison_metrics: Optional[ComparisonMetrics] = None,
        include_details: bool = True,
    ) -> str:
        """Generate a comprehensive evaluation report.
        
        Args:
            question: The original question
            evaluation_results: Results from evaluate_multiple()
            comparison_metrics: Optional comparison metrics
            include_details: Whether to include detailed category scores
            
        Returns:
            Report as markdown string
        """
        if comparison_metrics is None:
            comparison_metrics = self.compute_comparison_metrics(question, evaluation_results)
        
        report = "# Answer Evaluation Report\n\n"
        report += f"**Question:** {question}\n\n"
        
        # Summary section
        report += "## Summary\n\n"
        report += f"- **Models Evaluated:** {len(comparison_metrics.models_evaluated)}\n"
        report += f"- **Average Score:** {comparison_metrics.average_score:.2f}/10\n"
        report += f"- **Score Variance:** {comparison_metrics.score_variance:.2f}\n"
        report += f"- **Best Performer:** {comparison_metrics.highest_scoring_model} "
        report += f"({evaluation_results[comparison_metrics.highest_scoring_model]['overall_score']:.2f}/10)\n"
        report += f"- **Lowest Score:** {comparison_metrics.lowest_scoring_model} "
        report += f"({evaluation_results[comparison_metrics.lowest_scoring_model]['overall_score']:.2f}/10)\n\n"
        
        # Overall scores ranking
        report += "## Overall Scores (Ranked)\n\n"
        sorted_results = sorted(
            evaluation_results.items(),
            key=lambda x: x[1]["overall_score"],
            reverse=True
        )
        for i, (model_name, result) in enumerate(sorted_results, 1):
            score = result["overall_score"]
            report += f"{i}. **{model_name}**: {score:.2f}/10\n"
        report += "\n"
        
        # Detailed evaluations
        report += "## Detailed Evaluations\n\n"
        
        for model_name, result in sorted_results:
            report += f"### {model_name}\n\n"
            report += f"**Overall Score:** {result['overall_score']:.2f}/10\n\n"
            report += f"**Assessment:** {result['reasoning']}\n\n"
            
            # Category scores if available
            if include_details and "category_scores" in result:
                report += "**Category Breakdown:**\n"
                for category, score in sorted(
                    result["category_scores"].items(),
                    key=lambda x: x[1],
                    reverse=True
                ):
                    report += f"  - {category.replace('_', ' ').title()}: {score:.1f}/10\n"
                report += "\n"
            
            report += "\n"
        
        return report
    
    def save_results(
        self,
        evaluation_results: Dict[str, Dict[str, Any]],
        output_path: str,
        format: str = "json",
    ) -> None:
        """Save evaluation results to file.
        
        Args:
            evaluation_results: Results to save
            output_path: Path to save file
            format: 'json' or 'markdown'
        """
        if format == "json":
            with open(output_path, "w") as f:
                json.dump(evaluation_results, f, indent=2)
        else:
            raise ValueError(f"Unsupported format: {format}")


class BenchmarkSuite:
    """Suite for benchmarking multiple models on a set of questions."""
    
    def __init__(
        self,
        evaluator: AnswerEvaluator,
        questions: List[Dict[str, str]],
    ):
        """Initialize benchmark suite.
        
        Args:
            evaluator: AnswerEvaluator instance
            questions: List of question dictionaries with 'question' and optional 'id' keys
        """
        self.evaluator = evaluator
        self.questions = questions
        self.results = {}
    
    def benchmark(
        self,
        models_and_answers: Dict[str, Dict[str, str]],
        categories: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Run benchmark across multiple questions and models.
        
        Args:
            models_and_answers: Dict of model_name -> {question_id -> answer}
            categories: Evaluation categories
            
        Returns:
            Benchmark results with overall statistics
        """
        all_results = {}
        
        for question_data in self.questions:
            question = question_data.get("question")
            q_id = question_data.get("id", question[:50])  # Use first 50 chars as ID
            
            # Collect answers for this question from all models
            question_answers = {}
            for model_name, answers_dict in models_and_answers.items():
                if q_id in answers_dict:
                    question_answers[model_name] = answers_dict[q_id]
            
            if question_answers:
                # Evaluate all models on this question
                results = self.evaluator.evaluate_multiple(
                    question, question_answers, categories
                )
                all_results[q_id] = {
                    "question": question,
                    "results": results,
                }
        
        self.results = all_results
        return self._aggregate_results()
    
    def _aggregate_results(self) -> Dict[str, Any]:
        """Aggregate results across all questions."""
        model_scores = {}
        
        for question_id, question_results in self.results.items():
            for model_name, result in question_results["results"].items():
                if model_name not in model_scores:
                    model_scores[model_name] = []
                model_scores[model_name].append(result["overall_score"])
        
        # Calculate aggregate statistics
        aggregate = {}
        for model_name, scores in model_scores.items():
            aggregate[model_name] = {
                "mean_score": sum(scores) / len(scores),
                "min_score": min(scores),
                "max_score": max(scores),
                "num_questions": len(scores),
            }
        
        return {
            "per_question_results": self.results,
            "aggregate_statistics": aggregate,
            "overall_winner": max(aggregate.items(), key=lambda x: x[1]["mean_score"])[0],
        }
